import feedparser, json, os, re, requests
from datetime import datetime
from bs4 import BeautifulSoup
from pathlib import Path

USERNAME = "hosooinmymind"
RSS_URL = f"https://v2.velog.io/rss/@{USERNAME}"
POSTS_DIR = "_posts"
IMG_DIR = f"assets/images/{USERNAME}"
SITE_BASE_URL = "https://aneomagig.github.io"
HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Referer": "https://velog.io/",
    "Origin": "https://velog.io"
}

os.makedirs(POSTS_DIR, exist_ok=True)
os.makedirs(IMG_DIR, exist_ok=True)

feed = feedparser.parse(RSS_URL)
entries = sorted(feed.entries, key=lambda e: datetime(*e.published_parsed[:6]), reverse=True)

updated_posts = []

for entry in entries:
    title = entry.title.strip()
    slug = re.sub(r'[^a-zA-Z0-9ê°€-í£]+', '-', title).strip('-')
    date_parsed = datetime(*entry.published_parsed[:6])
    date_str = date_parsed.strftime("%Y-%m-%d %H:%M:%S +0900")
    date_filename = date_parsed.strftime("%Y-%m-%d")
    filename = f"{POSTS_DIR}/{date_filename}-{slug}.md"
    file_path = Path(filename)

    # RSS ë³¸ë¬¸ (ì¸ë„¤ì¼ + ë‚´ìš© ì¼ë¶€)
    soup = BeautifulSoup(entry.description, "html.parser")

    # ğŸ¯ 1ï¸âƒ£ HTML í˜ì´ì§€ ìš”ì²­ (ì‹œë¦¬ì¦ˆ, íƒœê·¸ ê°€ì ¸ì˜¤ê¸°)
    html_url = entry.link
    html_res = requests.get(html_url, headers=HEADERS)
    html_soup = BeautifulSoup(html_res.text, "html.parser")

    # ğŸ”¹ ì‹œë¦¬ì¦ˆëª… ì¶”ì¶œ
    series_tag = html_soup.find("a", href=lambda x: x and "/series/" in x)
    series_name = series_tag.text.strip() if series_tag else None

    # ğŸ”¹ íƒœê·¸ ì¶”ì¶œ
    tag_elements = html_soup.select("a.tag-item") or []
    tags = [t.text.strip().replace("#", "") for t in tag_elements]

    # ğŸ”¹ ë³¸ë¬¸ ë‚´ìš© (RSS ìš”ì•½ ëŒ€ì‹  ì „ì²´ HTML)
    content_div = html_soup.select_one("div.velog__post-content")
    if content_div:
        soup = content_div

    # ğŸ¯ 2ï¸âƒ£ ì´ë¯¸ì§€ ë¡œì»¬ ì €ì¥ (ê²½ë¡œ êµ¬ì¡° ìœ ì§€)
    for img in soup.find_all("img"):
        img_url = img.get("src")
        if not img_url or not img_url.startswith("http"):
            continue

        rel_path = img_url.split("https://velog.velcdn.com/")[-1]
        local_path = os.path.join(IMG_DIR, rel_path)
        os.makedirs(os.path.dirname(local_path), exist_ok=True)

        if not os.path.exists(local_path):
            r = requests.get(img_url, headers=HEADERS)
            if r.status_code == 200:
                with open(local_path, "wb") as f:
                    f.write(r.content)
                print(f"ğŸ“¸ Saved {rel_path}")
            else:
                print(f"âš ï¸ Failed ({r.status_code}): {img_url}")

        img["src"] = f"/{local_path.replace(os.sep, '/')}"

    # ğŸ¯ 3ï¸âƒ£ Markdown íŒŒì¼ ìƒì„± (ì‹œë¦¬ì¦ˆ + íƒœê·¸ í¬í•¨)
    front_matter = f"""---
layout: post
title: "{title}"
date: {date_str}
categories: velog
"""

    if series_name:
        front_matter += f'series: "{series_name}"\n'

    if tags:
        front_matter += "tags:\n"
        for t in tags:
            front_matter += f"  - {t}\n"

    front_matter += "---\n\n"

    markdown = front_matter + str(soup)

    if file_path.exists():
        with file_path.open("r", encoding="utf-8") as existing_file:
            if existing_file.read() == markdown:
                print(f"â­ï¸ Skipped unchanged post: {filename}")
                continue

    with file_path.open("w", encoding="utf-8") as f:
        f.write(markdown)

    print(f"âœ… Saved post: {filename} ({series_name or 'No series'})")

    post_info = {
        "title": title,
        "slug": slug,
        "date": date_parsed.isoformat(),
        "filename": str(file_path),
        "velog_url": entry.link,
        "site_url": f"{SITE_BASE_URL}/velog/{date_parsed.strftime('%Y/%m/%d')}/{slug}.html"
    }
    updated_posts.append(post_info)

latest_info_path = Path("latest_post.json")
if updated_posts:
    updated_posts.sort(key=lambda p: p["date"], reverse=True)
    with latest_info_path.open("w", encoding="utf-8") as f:
        json.dump(updated_posts[0], f, ensure_ascii=False, indent=2)
    print(f"ğŸ†• Latest synced post: {updated_posts[0]['title']}")
elif latest_info_path.exists():
    latest_info_path.unlink()

print("ğŸ‰ ëª¨ë“  í¬ìŠ¤íŠ¸ RSS + HTML ë³‘í–‰ í¬ë¡¤ë§ ì™„ë£Œ!")
